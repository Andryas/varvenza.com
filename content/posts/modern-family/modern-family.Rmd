---
title: Modern Family
author: Andryas Wavrzenczak
date: '2022-11-11'
slug: "modern-family"
draft: true
tags: ["text-mining", "tv-show", "modern-family", "R"]
---

```{r setup, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  eval = FALSE,
  fig.height = 9,
  fig.width = 11
)
options(dplyr.print_max = 100)

# knitr::purl(
#     input = "/Users/andryaswavrzenczak/Documents/blog/content/post/modern-family/modern-family.Rmd",
#     output = "/Users/andryaswavrzenczak/Documents/blog/content/post/modern-family/modern-family.R",
#     documentation = 0
# )

# options(blogdown.method = "markdown")
```

<!-- https://towardsdatascience.com/text2emotion-python-package-to-detect-emotions-from-textual-data-b2e7b7ce1153 -->
<!-- https://towardsdatascience.com/sentiment-analysis-of-the-lead-characters-on-f-r-i-e-n-d-s-51aa5abf1fa6 -->

Modern Family, for sure, is my favourite TV Show. I love how it happens, with the little
misunderstandings at the beginning of each episode, talking about situations that go
unnoticed by us most of the time and are dissolved in magical ways. It is a psychological
session that shows you circumstances you probably had passed through and makes you see
from different perspectives the ordinary moments that you and I, ordinary people, live in
our daily lives.

Sometimes you act like Phill, sometimes you are more likely Jay, or sometimes you speak
like Gloria (My case these last months, rs).

Doesn't matter how it happens, it makes you grow emotionally.

Thatâ€™s said, letâ€™s have some fun time with data. What motivated this post it was a random
thought that had couple of weeks agoâ€¦

<div class='alert danger no-icon'>ðŸ’­ How can I describe which character in Modern Family as the soccer players in my video game when I was a kid?</div>

<br/>
<br/>

![https://www.mobygames.com/images/shots/l/328750-winning-eleven-pro-evolution-soccer-2007-windows-screenshot.jpg](https://www.mobygames.com/images/shots/l/328750-winning-eleven-pro-evolution-soccer-2007-windows-screenshot.jpg)

Guess what. I can! And how is it works? Well, as follow:

1. Find a well structured script for all lines of modern family
2. Extract and tabulate this data
3. Apply a sentimental model to the data
4. Summary the data and create beautiful visualization

What you can expect from this is see how to extract information from a PDF and make it
tabular, how to use a pre-trained model from Python inside R environment, how to aggregate
and manipulate the data in a specific way to facilitate the further beautiful
visualizations.

Letâ€™s start!

If you want all the code to explore by yourself, [here will go](../modern-family.R)

# Libs

```{r libs, eval=TRUE}
library(tidyverse)
library(pdftools)
library(gt)
library(tidytext)
library(rvest)
library(echarts4r)

library(reticulate)
use_virtualenv("r-reticulate")
use_python("/Users/andryaswavrzenczak/Documents/blog/venv/bin/python")
```

```{r, echo=FALSE,eval=TRUE}
load("/Users/andryaswavrzenczak/Documents/blog/content/post/modern-family/modern_family.RData")
```


# Getting the data

Searching on Google about scripts related to the first Modern Family season, I found this
link here.

![https://res.cloudinary.com/andryas/image/upload/v1666572916/blog/search_google.png](https://res.cloudinary.com/andryas/image/upload/v1666572916/blog/search_google.png)

You can use the code below to download the pdf using `R`. 

```{r getting_data, eval = FALSE}
download.file(
  "https://blog.kakaocdn.net/dn/CnVbv/btqUmJjbuWJ/OfaJTFKkUorXxXKxIYnuB1/tfile.pdf",
  "~/Documents/data/modern-family/script_season_1.pdf" ## path for a place in your computer
)
```

# Preparing the data

Using the `pdf_text`, we input the data inside R as a single column in a `data.frame`.
After that, we extract some necessary information to structure the data using' regex'.

As shown in the image below, some patterns are all over the script. What we are going to
do is transform this big script, written in two columns, into one long sequenced script.

![How the script is](https://res.cloudinary.com/andryas/image/upload/v1666572917/blog/script.png)

```{r preparing1}
data <-
  tibble(text = pdf_text(
    stringr::str_interp("~/Documents/data/modern-family/script_season_1.pdf")
  ))
```


```{r, eval=TRUE}
data
```


```{r preparing1_1}
data <- data |>
  mutate(
    ep = str_extract(text, "(?<=Season 1x)[0-9]{2}"),
    page = trimws(str_extract(text, "(?<=page)\\s?[0-9]+"))
  )

data2 <- data |>
  group_by(ep, page) |>
  group_split() |>
  map(function(.x) {
    text <- str_c(.x$text, collapse = "")
    text <- str_split(text, "\n")[[1]]

    x <-
      map(str_split(text, regex("\\s{3,100}")), function(.w) {
        if (length(.w) == 1 && .w == "") {
          tibble(c1 = "", c2 = "")
        } else if (length(.w) == 1) {
          tibble(c1 = .w[1], c2 = "")
        } else if (length(.w) == 3) {
          tibble(c1 = .w[1], c2 = .w[3])
        } else {
          tibble(c1 = .w[1], c2 = .w[2])
        }
      }) |>
      bind_rows()

    c1 <-
      str_replace(x$c1,
        "(?<=^\\w{1,15})\\s?:\\s+|(?<=^\\w{1,15} \\w{1,15})\\s?:\\s+",
        ": ")
    c1 <-
      str_replace(c1,
        "(?<=^\\w{1,15})\\s?;\\s+|(?<=^\\w{1,15} \\w{1,15})\\s?;\\s+",
        ": ")

    c2 <-
      str_replace(x$c2,
        "(?<=^\\w{1,15})\\s?:\\s+|(?<=^\\w{1,15} \\w{1,15})\\s?:\\s+",
        ": ")
    c2 <-
      str_replace(c2,
        "(?<=^\\w{1,15})\\s?;\\s+|(?<=^\\w{1,15} \\w{1,15})\\s?;\\s+",
        ": ")

    cs <- c(c1, c2)

    tibble(script = cs,
      ep = unique(.x$ep),
      page = unique(.x$page))

  }) |>
  bind_rows()
```

```{r, eval=TRUE}
data2
```


## Removing unnecessary lines

Here we clean empty and unnecessary lines.

```{r preparing2}
data3 <- data2 |>
  filter(str_detect(script, "Modern Family Season|OPENING CREDITS", negate = TRUE) &
    script != "")
```

## Fixing characters lines

As I will show you, there are many typing errors. Still, we will focus only on referents
to the characters and the main characters. For instance, using Cameron, we have Cam,
Cameron and Cams, all are the same character.

First, we set each line to the respective character. It is done by extracting the first
words before **:**. But as not all lines have this standard, we paste the rows without
**:** to the first previous row that starts with it.

```{r preparing3}
data4 <- data3 |>
  mutate(
    script = str_replace(script, "\\s+:\\s+", ": "),
    script = str_replace(script, "\\s+;\\s+", "; "),
    not_a_line = str_detect(script, "[\\w\\s]+(?=: )", negate = TRUE),
    not_a_line = ifelse(script == "", FALSE, not_a_line)
  )
```

```{r, eval=TRUE}
## For example ##
data4 |>
  slice(30:34)
```

```{r preparing3_1}
data4 <- data4 |>
  mutate(
    script = str_replace(script, "(?<=\\w)\\s+:\\s+", ":"),
    script = str_replace(script, "(?<=\\w)\\s+;\\s+", "; "),
    not_a_line = str_detect(script, "[A-z]+(?=:)", negate = TRUE),
    not_a_line = ifelse(script == "", FALSE, not_a_line)
  )

missing_lines <- which(data4$not_a_line == TRUE)
for (i in rev(missing_lines)) {
  data4$script[i - 1] <- str_c(data4$script[i - 1], " ", data4$script[i])
}
data4 <- data4[-missing_lines, ]
data4 <- data4 |>
  select(-not_a_line)
```

```{r, eval=TRUE}
data4 |>
  slice(22:23)
```

Second, we identify each character's name, select the main characters apply a distance
string `Jaro-Winkler` to fix some cases of misspelling.

<div class='alert warning no-icon'>I also add a new column, `script2`, doing the standard text mining process, tolower, remove accents, stopwords, punctuation etc etc...</div>

```{r preparing4}
data5 <- data4 |>
  filter(script != "") |>
  mutate(
    character = str_extract(script, "[\\w\\s]+(?=:)"),
    character = trimws(tolower(character)),

    script2 = trimws(str_replace(script, "^[\\w\\s]+:", "")),
    line = script2,
    line_length = nchar(script2)
  ) |>
  select(page, ep, character, line, script)

main_characters <- data5 |>
  count(character, sort = TRUE) |>
  mutate(pct = n / sum(n), pct_acc = cumsum(pct)) |>
  filter(pct_acc <= 0.9) |>
  pull(character)

other_characters <- data5 |>
  count(character, sort = TRUE) |>
  mutate(pct = n / sum(n), pct_acc = cumsum(pct)) |>
  filter(pct_acc > 0.9) |>
  pull(character)

fix_characters <- map(main_characters, function(.x) {
  other_characters[stringdist::stringdist(.x, other_characters, "jw") <= 0.15]
}) |>
  setNames(main_characters)

fix_characters <- fix_characters[which(sapply(fix_characters, length) > 0)]
fix_characters[[3]] <- fix_characters[[3]][-1]

for (i in 1:length(fix_characters)) {
  for (j in 1:length(fix_characters[[i]])) {
    data5$character[data5$character == fix_characters[[i]][[j]]] <-
      names(fix_characters)[i]
  }
}
```

```{r, eval=TRUE}
data5 |>
  slice(1:10) |>
  gt(caption = "Ten first rows of the data.frame with additional informations") |>
  cols_align(
    align = "center",
    columns = c(ep, page, character)
  ) |>
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_borders(sides = "bottom", weight = px(3)),
      cell_text(weight = "bold")
    )
  )
```


# Mining the lines, getting the feelings

Now one of the most important parts, we will extract feelings for each character's line.

(I remember the first time I did this in a text-mining course four years ago, we prepared
all the data with the standard preprocess, and we created dictionaries for the unique
feeling of each word, positive, negative or neutral, and then we added the values to see
how positive/negative was the text. Good times.)

Now life is more accessible. To have more challenges along the way, we will do the
sentimental analysis using a pre-trained model from TensorFlow. That's right, a model in
built-in python applied inside an R environment.


```{r sent1, message=FALSE, warning=FALSE}
transformers <- import("transformers")
classifier <- transformers$pipeline(
  "text-classification",
  model = 'bhadresh-savani/distilbert-base-uncased-emotion',
  return_all_scores = TRUE)

# Applying the model for each character's line

data6 <- data5 |>
  mutate(sent = classifier(line))

data7 <- data6 |>
  bind_cols(map(data6$sent, function(.x) {
    tibble(variable = unlist(map(.x, "label")),
      score = unlist(map(.x, "score"))) |>
      spread(variable, score)
  }) |>
    bind_rows()) |>
  select(-sent)
```

```{r sent2, eval=TRUE}
data7 |>
  slice(1:10) |>
  select(-page, -ep, -script) |>
  gt(caption = "Ten first rows with the results from the pre-trained model") |>
  cols_align(align = "center", ) |>
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_borders(sides = "bottom", weight = px(3)),
      cell_text(weight = "bold")
    )
  ) |>
  fmt_percent(columns = c(anger, fear, joy, love, sadness, surprise))
```


# Beautiful visualization

Now let's transform all this data into something meaningful, but before...

## Exploring the common words of each character

The general idea here is to find the words that characterize each character. For
that, we will use the **tf-idf**, which is intended to measure how important a
word is to a document in a collection (or corpus) of documents, for example, to
one novel in a collection of novels or to one website in a collection of
websites.  [Text Mining with R](https://www.tidytextmining.com/tfidf.html#:~:text=is%20intended%20to%20measure%20how%20important%20a%20word%20is%20to%20a%20document%20in%20a%20collection%20(or%20corpus)%20of%20documents%2C%20for%20example%2C%20to%20one%20novel%20in%20a%20collection%20of%20novels%20or%20to%20one%20website%20in%20a%20collection%20of%20websites.)

In our context, the documents are the characters.

```{r sent3}
unigram <- data7 |>
  filter(character %in% main_characters) |>
  select(character, line) |>
  unnest_tokens(word, line) |>
  count(character, word) |>
  bind_tf_idf(word, character, n) |>
  filter(!word %in% stop_words$word) |>
  drop_na() |>
  arrange(desc(n)) |>
  ungroup()
```

Now we have the **tf-idf** for each word of our characters; let's move forward!

## Finally, the output

Here is a little experiment on how I would like to summarize each character.

```{r, eval=TRUE}
i <- 1


## I got some pictures for each character from google images ##
main_characters_img <- c(
  "https://res.cloudinary.com/andryas/image/upload/v1668181537/blog/maxresdefault.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195080/blog/claire-face-feature-modern-family.avif",
  "https://res.cloudinary.com/andryas/image/upload/v1668195382/blog/a3062bb3-5115-4a88-8b2e-b06e39575f3a.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195381/blog/maxresdefault2.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195380/blog/Cam-Tucker-750x402.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195379/blog/cda.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195379/blog/98.webp",
  "https://res.cloudinary.com/andryas/image/upload/v1668195377/blog/MV5BNTI1NTEyNWEtNDdiYi00MzZiLTgxNGQtZjJkMmZkN2I4NDk3XkEyXkFqcGdeQXVyNTM3MDMyMDQ_._V1_.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195484/blog/bea72840602ecfce51f2d52bf0a7bd8e.png",
  "https://res.cloudinary.com/andryas/image/upload/v1668195376/blog/modern-family-cast-season-1-ariel-winter-1565016010.jpg",
  "https://res.cloudinary.com/andryas/image/upload/v1668195365/blog/dylan-modern-family-musicas.jpg"
)


plot_sent <- data7 |>
  filter(character == !!main_characters[i]) |>
  summarise_at(vars(anger, fear, joy, love, sadness, surprise), mean) |>
  gather(variable, value) |>
  e_charts(variable) |>
  e_radar(
    value,
    max = 0.6,
    areaStyle = list(),
    legend = FALSE
  ) |>
  e_tooltip(trigger = "item")

plot_unigram <- unigram |>
  filter(character == !!main_characters[i]) |>
  select(word, tf_idf) |>
  arrange(desc(tf_idf)) |>
  slice(1:100) |>
  e_color_range(tf_idf, color) |>
  e_charts() |>
  e_cloud(word, tf_idf, color, shape = "circle", ) |>
  e_title(str_c(main_characters[i], " - most used words"))

shiny::fluidRow(
  shiny::fluidRow(
    style = "width: 100%",
    class = "align-items-center",
    shiny::column(6, shiny::img(src = main_characters_img[i], height = 300, width = 300)),
    shiny::column(6, htmltools::div(plot_sent)),
  ),
  shiny::fluidRow(
    style = "width: 100%",
    plot_unigram
  )
)
```

To be honest I didn't like so much the radar plot, I will be using a different one, check
it out.

```{r, eval = TRUE}
tags <- map(1:length(main_characters), function(i) {
  plot_sent <- data7 |>
    filter(character == !!main_characters[i]) |>
    summarise_at(vars(anger, fear, joy, love, sadness, surprise), mean) |>
    gather(variable, value) |>
    e_charts(variable) |>
    e_pie(value, roseType = "radius", legend = FALSE)

  plot_unigram <- unigram |>
    filter(character == !!main_characters[i]) |>
    select(word, tf_idf) |>
    arrange(desc(tf_idf)) |>
    slice(1:100) |>
    e_color_range(tf_idf, color) |>
    e_charts() |>
    e_cloud(word, tf_idf, color, shape = "circle", ) |>
    e_title(str_c(main_characters[i], " - most used words"))

  shiny::fluidRow(
    shiny::fluidRow(
      style = "width: 100%",
      class = "align-items-center",
      shiny::column(6, shiny::img(src = main_characters_img[i], height = 300, width = 300)),
      shiny::column(6, htmltools::div(plot_sent)),
    ),
    shiny::fluidRow(
      style = "width: 100%",
      plot_unigram
    )
  )
})

shiny::tagList(tags)
```

## Final words

The results were different from what I expected. I thought it would be more variation
between the feeling. The feeling is basically switching between joy and anger, most of the
time.

Gloria, one of the angriest characters, had one of the lowest scores for this feeling,
which is kind of weird.

This model, in specific,  is not appropriate for character lines; it would be better if I
aggregated the lines into scenes and then applied the model. The result would be an
episode analysis, not the character itself.

But that is it. It will stay for the next one.

Hasta la vista.

